{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e91905",
   "metadata": {},
   "source": [
    "# Text Chunking, Embedding, and Vector Store Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa61b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"../data/processed/filtered_complaints.csv\")\n",
    "df = df[['Complaint ID', 'Mapped Product', 'cleaned_narrative']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bef8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['cleaned_narrative']\n",
    "    chunked_texts = text_splitter.split_text(text)\n",
    "    for chunk in chunked_texts:\n",
    "        chunks.append({\n",
    "            'chunk': chunk,\n",
    "            'complaint_id': row['Complaint ID'],\n",
    "            'product': row['Mapped Product']\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "chunk_df = pd.DataFrame(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f763c326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a xxxx xxxx card was opened under my name by a...</td>\n",
       "      <td>14069121</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and immediately closed the card however they h...</td>\n",
       "      <td>14069121</td>\n",
       "      <td>Credit card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i made the mistake of using my wellsfargo debi...</td>\n",
       "      <td>14061897</td>\n",
       "      <td>Savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went into the branch and was told they could...</td>\n",
       "      <td>14061897</td>\n",
       "      <td>Savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i waited a few days and got a letter stating m...</td>\n",
       "      <td>14061897</td>\n",
       "      <td>Savings account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  complaint_id  \\\n",
       "0  a xxxx xxxx card was opened under my name by a...      14069121   \n",
       "1  and immediately closed the card however they h...      14069121   \n",
       "2  i made the mistake of using my wellsfargo debi...      14061897   \n",
       "3  i went into the branch and was told they could...      14061897   \n",
       "4  i waited a few days and got a letter stating m...      14061897   \n",
       "\n",
       "           product  \n",
       "0      Credit card  \n",
       "1      Credit card  \n",
       "2  Savings account  \n",
       "3  Savings account  \n",
       "4  Savings account  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728e8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.to_csv('../data/processed/text_chunks.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18bfe86",
   "metadata": {},
   "source": [
    "# Load Model and Encode Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87dd1231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               chunk  complaint_id  \\\n",
      "0  a xxxx xxxx card was opened under my name by a...      14069121   \n",
      "1  and immediately closed the card however they h...      14069121   \n",
      "2  i made the mistake of using my wellsfargo debi...      14061897   \n",
      "3  i went into the branch and was told they could...      14061897   \n",
      "4  i waited a few days and got a letter stating m...      14061897   \n",
      "\n",
      "           product  \n",
      "0      Credit card  \n",
      "1      Credit card  \n",
      "2  Savings account  \n",
      "3  Savings account  \n",
      "4  Savings account  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preview the content\n",
    "print(chunk_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b770a2db-1a04-431b-9bb3-06f7982e3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsem\\AppData\\Local\\Temp\\ipykernel_28460\\2842432089.py:15: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subset = text_chunk.groupby('product').apply(lambda x: sample_chunks(x, max_per_product)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa86d5b325b417591d6ac5f4b528376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to Chroma vector store...\n",
      "✅ Embeddings and metadata successfully stored in vector_store.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your chunk data CSV\n",
    "text_chunk = pd.read_csv(\"../data/processed/text_chunks.csv\")\n",
    "\n",
    "# Function to sample chunks\n",
    "def sample_chunks(group, max_samples):\n",
    "    return group.sample(n=min(len(group), max_samples), random_state=42)\n",
    "\n",
    "# Sample chunks from each product\n",
    "max_per_product = 1250  # Adjust this number to get closer to 5000 total\n",
    "subset = text_chunk.groupby('product').apply(lambda x: sample_chunks(x, max_per_product)).reset_index(drop=True)\n",
    "\n",
    "# If the total is less than 5000, sample more randomly from the entire dataset\n",
    "while len(subset) < 5000:\n",
    "    additional_samples = text_chunk.sample(n=5000 - len(subset), random_state=42)\n",
    "    subset = pd.concat([subset, additional_samples]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Limit to 5000 chunks if necessary\n",
    "subset = subset.sample(n=min(len(subset), 5000), random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Initialize Chroma client (persistent storage)\n",
    "client = PersistentClient(path=\"../vector_store\")\n",
    "collection = client.get_or_create_collection(\"complaints\")\n",
    "\n",
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare data\n",
    "texts = subset['chunk'].tolist()\n",
    "ids = [f\"{row['complaint_id']}_{i}\" for i, row in subset.iterrows()]\n",
    "metadatas = subset[['complaint_id', 'product']].to_dict(orient='records')\n",
    "\n",
    "# Generate embeddings with progress bar\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(texts, batch_size=32, show_progress_bar=True).tolist()\n",
    "\n",
    "# Add data to Chroma vector store\n",
    "print(\"Saving to Chroma vector store...\")\n",
    "collection.add(\n",
    "    documents=texts,\n",
    "    embeddings=embeddings,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(\"✅ Embeddings and metadata successfully stored in vector_store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a876a-2d4a-43fc-8e92-05d279f44636",
   "metadata": {},
   "source": [
    "# Task 2: Chunking and Embedding Report\n",
    "\n",
    "## Introduction\n",
    "In this task, I focused on processing customer complaints by chunking them into manageable parts and generating vector embeddings. These embeddings will be used later for retrieval and generation tasks in our Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "## Data Preparation\n",
    "### Loading the Dataset\n",
    "I began by loading the chunk data from a CSV file that contains processed text chunks of customer complaints.\n",
    "\n",
    "### Sampling\n",
    "Given that the dataset contained over 78,000 chunks, I decided to sample a subset of 5,000 chunks to make the processing more efficient. To ensure a balanced representation, I sampled from each product category, limiting the maximum number of chunks per category.\n",
    "\n",
    "## Embedding Generation\n",
    "### Model Selection\n",
    "I chose the `sentence-transformers/all-MiniLM-L6-v2` model for generating embeddings. This model was selected for its balance between performance and computational efficiency, making it suitable for embedding tasks.\n",
    "\n",
    "### Generating Embeddings\n",
    "I generated embeddings for each text chunk using the selected model. These embeddings are crucial for enabling similarity searches in the subsequent RAG pipeline.\n",
    "\n",
    "## Storing in Vector Store\n",
    "### Vector Store Initialization\n",
    "I utilized ChromaDB as my vector store for embedding storage, ensuring that I could efficiently retrieve embeddings later.\n",
    "\n",
    "### Saving Embeddings\n",
    "I stored the embeddings alongside their corresponding metadata, such as complaint ID and product category, in the vector store. This step is essential for tracing the embeddings back to their original source.\n",
    "\n",
    "## Conclusion\n",
    "By chunking the data and generating embeddings, I laid the groundwork for my RAG pipeline. The embeddings are now stored in a vector database, ready for retrieval and further analysis in upcoming tasks.\n",
    "\n",
    "## Future Work\n",
    "The next steps involve implementing the retrieval and generation logic, evaluating the effectiveness of my pipeline, and refining my prompt engineering to enhance response quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
