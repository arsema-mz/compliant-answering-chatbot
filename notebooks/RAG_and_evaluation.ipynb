{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a08aba2-506b-493d-af07-8231f8adffff",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b49e687-19da-418b-ae30-6cdc86f471c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\arsem\\Desktop\\compliant-answering-chatbot\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Reload the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e130d788-050c-4b58-9fba-2a2b704b47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(question, model, collection, k=5):\n",
    "    # Step 1: Embed the question\n",
    "    question_embedding = model.encode(question)\n",
    "\n",
    "    # Step 2: Perform similarity search in the vector store\n",
    "    results = collection.query(question_embedding.tolist(), n_results=k)\n",
    "\n",
    "    # Step 3: Extract relevant chunks and metadata\n",
    "    retrieved_chunks = results['documents'][0]  # Access the first element of the 'documents' list\n",
    "    retrieved_ids = [meta['complaint_id'] for meta in results['metadatas'][0]]  # Extract complaint IDs\n",
    "\n",
    "    return retrieved_chunks, retrieved_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2462fc1d-0299-4a41-9cad-9a9545cb31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "i am filing a complaint against capital one regarding the deceptive practices related to their 360  high yield interest  savings accounts capital one misrepresented the 360 savings account as having one of the nation s highest interest rates yet failed to notify accountholders about the superior\n",
      "their response to my complaint basically states it doesnt matter prove it and bank at your own risk money isnt safe no wonder so many people refuse to have a bank account this is unjust to hold onto money that isnt theres closing my account has caused more harm to me im expecting my tax refund to a\n",
      "closing statement consumers have a right to be informed about changes that impact their finances even if prior notice is not required wises failure to provide any direct notification about the rate change demonstrates a lack of transparency and potential noncompliance with the truth in savings act\n",
      "i have not received a response at this moment i do not have much hope this company will respond to me and answer my questions the question of why and where is the bank account included in the information they must retain for their legal and compliance purposes has gone unanswered if they can not\n",
      "i do want to note that the local branch has been wonderful they have tried hard to figure out how to deal with this but utilmately it is out of their hands \n",
      "\n",
      "i then called consumer financial protection bureau and am filing a complaint\n",
      "Retrieved IDs:\n",
      "[11560754, 8407117, 11229959, 7975496, 3774288]\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "client = PersistentClient(path=\"../vector_store\")\n",
    "collection = client.get_collection(\"complaints\")  # Replace with your collection name\n",
    "\n",
    "# Define the testing function\n",
    "def test_retrieve_function(question):\n",
    "    retrieved_chunks, retrieved_ids = retrieve_relevant_chunks(question, model, collection)\n",
    "    print(\"Retrieved Chunks:\")\n",
    "    for chunk in retrieved_chunks:\n",
    "        print(chunk)\n",
    "    print(\"Retrieved IDs:\")\n",
    "    print(retrieved_ids)\n",
    "\n",
    "# Example question to test the retriever\n",
    "test_question = \"What are the main complaints about savings account?\"\n",
    "test_retrieve_function(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c406aad-2089-4e23-a019-fe8b15a420d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def generate_answer(question, retrieved_chunks, model, tokenizer):\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # for GPT-2 padding\n",
    "    \n",
    "    prompt_template = (\n",
    "        \"You are a financial analyst assistant for CrediTrust. \"\n",
    "        \"Your task is to answer questions about customer complaints. \"\n",
    "        \"Use the following retrieved complaint excerpts to formulate your answer. \"\n",
    "        \"If the context doesn't contain the answer, state that you don't have enough information. \"\n",
    "        \"Context: {context} \"\n",
    "        \"Question: {question} \"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs['input_ids'].shape[1] + 100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e26913d2-651c-4167-845b-05c30be5b150",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 384, got 9",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m test_question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the main complaints about product A?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# collection and retrieve_relevant_chunks should be defined in your code\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtest_rag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtest_rag_pipeline\u001b[39m\u001b[34m(test_question, model, tokenizer, collection)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_rag_pipeline\u001b[39m(test_question, model, tokenizer, collection):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Step 1: Retrieve relevant chunks (you provide your own retrieve_relevant_chunks)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     retrieved_chunks, retrieved_ids = \u001b[43mretrieve_relevant_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Step 2: Generate answer using the pre-loaded model and tokenizer\u001b[39;00m\n\u001b[32m      6\u001b[39m     answer = generate_answer(test_question, retrieved_chunks, model, tokenizer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mretrieve_relevant_chunks\u001b[39m\u001b[34m(question, model, collection, k)\u001b[39m\n\u001b[32m      3\u001b[39m question_embedding = tokenizer.encode(question, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Step 2: Perform similarity search in the vector store\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Assuming results is a list of retrieved chunks and their IDs\u001b[39;00m\n\u001b[32m      9\u001b[39m retrieved_chunks = [result[\u001b[33m'\u001b[39m\u001b[33mchunk\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\compliant-answering-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:221\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m    187\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    210\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    211\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m     include=include,\n\u001b[32m    219\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    234\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    235\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\compliant-answering-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\rust.py:505\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    489\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    491\u001b[39m     CollectionQueryEvent(\n\u001b[32m    492\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m     )\n\u001b[32m    503\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    518\u001b[39m     ids=rust_response.ids,\n\u001b[32m    519\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    525\u001b[39m     distances=rust_response.distances,\n\u001b[32m    526\u001b[39m )\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Collection expecting embedding with dimension of 384, got 9"
     ]
    }
   ],
   "source": [
    "def test_rag_pipeline(test_question, model, tokenizer, collection):\n",
    "    # Step 1: Retrieve relevant chunks (you provide your own retrieve_relevant_chunks)\n",
    "    retrieved_chunks, retrieved_ids = retrieve_relevant_chunks(test_question, model, collection)\n",
    "    \n",
    "    # Step 2: Generate answer using the pre-loaded model and tokenizer\n",
    "    answer = generate_answer(test_question, retrieved_chunks, model, tokenizer)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Retrieved Chunks:\")\n",
    "    for chunk in retrieved_chunks:\n",
    "        print(chunk)\n",
    "    \n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "# Usage example:\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "test_question = \"What are the main complaints about product A?\"\n",
    "# collection and retrieve_relevant_chunks should be defined in your code\n",
    "\n",
    "test_rag_pipeline(test_question, model, tokenizer, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ca68e-2a89-4992-b5e2-e8613d8e7713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
